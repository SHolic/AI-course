{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## COSC-4117EL: Gridworld - Q-Learning (with epsilon-greedy)\n",
        "\n",
        "\n",
        "This sample shows the implementation of Q-learning (a model-free reinforcement learning algorithm) applied to a simple grid world. The agent can take four possible actions at each state: move up, down, left, or right. The Q-values represent the expected cumulative reward the agent can obtain by taking a specific action in a specific state.\n",
        "\n",
        "**Q-learning with epsilon-greedy exploration:**\n",
        "\n",
        "For each state (i, j), the algorithm iterates over possible actions.\n",
        "It then calculates the resulting state (ni, nj) from taking an action.\n",
        "The agent decides whether to explore (choose a random action) or exploit (choose the action with the highest Q-value for the state) based on the epsilon value.\n",
        "Q-value for the current state and action is updated using the Q-learning update rule.\n",
        "\n",
        "**Q-value Clipping:**\n",
        "\n",
        "After each Q-value update, the code ensures that the Q-value for the action at state (0,1) is at most 10, and the Q-value for the action at state (0,2) is at least -10.\n",
        "\n",
        "The Q-values represent the agent's learned values for how good each action is for each state, considering future rewards.\n",
        "\n",
        "When you run this code, it will give you the Q-values after a specified number of iterations. If you increase the number of iterations or adjust the hyperparameters, the Q-values might converge to more optimal values, depending on the problem and reward structure.\n",
        "\n",
        "# `TODO: can you add a living reward of -1 for each step the agent takes?`"
      ],
      "metadata": {
        "id": "SDscE5n1Kxmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIti3AXYKtNo",
        "outputId": "3026af48-14c7-4b6b-fd67-79d00797abc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-values after 10 iteration(s)\n",
            "State (0,0): (-1, 0) Q=9.73\n",
            "State (0,0): (1, 0) Q=9.43\n",
            "State (0,0): (0, -1) Q=9.84\n",
            "State (0,0): (0, 1) Q=9.98\n",
            "-----------------\n",
            "State (0,1): (-1, 0) Q=10.00\n",
            "State (0,1): (1, 0) Q=10.00\n",
            "State (0,1): (0, -1) Q=10.00\n",
            "State (0,1): (0, 1) Q=10.00\n",
            "-----------------\n",
            "State (0,2): (-1, 0) Q=-10.00\n",
            "State (0,2): (1, 0) Q=-10.00\n",
            "State (0,2): (0, -1) Q=-10.00\n",
            "State (0,2): (0, 1) Q=-10.00\n",
            "-----------------\n",
            "State (1,0): (-1, 0) Q=9.77\n",
            "State (1,0): (1, 0) Q=8.95\n",
            "State (1,0): (0, -1) Q=9.72\n",
            "State (1,0): (0, 1) Q=9.79\n",
            "-----------------\n",
            "State (1,1): (-1, 0) Q=9.98\n",
            "State (1,1): (1, 0) Q=9.63\n",
            "State (1,1): (0, -1) Q=9.65\n",
            "State (1,1): (0, 1) Q=9.53\n",
            "-----------------\n",
            "State (1,2): (-1, 0) Q=-9.98\n",
            "State (1,2): (1, 0) Q=8.73\n",
            "State (1,2): (0, -1) Q=9.85\n",
            "State (1,2): (0, 1) Q=9.69\n",
            "-----------------\n",
            "State (2,0): (-1, 0) Q=9.61\n",
            "State (2,0): (1, 0) Q=8.25\n",
            "State (2,0): (0, -1) Q=9.28\n",
            "State (2,0): (0, 1) Q=9.62\n",
            "-----------------\n",
            "State (2,1): (-1, 0) Q=9.92\n",
            "State (2,1): (1, 0) Q=9.23\n",
            "State (2,1): (0, -1) Q=9.29\n",
            "State (2,1): (0, 1) Q=9.15\n",
            "-----------------\n",
            "State (2,2): (-1, 0) Q=9.29\n",
            "State (2,2): (1, 0) Q=8.63\n",
            "State (2,2): (0, -1) Q=9.82\n",
            "State (2,2): (0, 1) Q=9.60\n",
            "-----------------\n",
            "State (3,0): (-1, 0) Q=9.29\n",
            "State (3,0): (1, 0) Q=8.77\n",
            "State (3,0): (0, -1) Q=8.36\n",
            "State (3,0): (0, 1) Q=9.29\n",
            "-----------------\n",
            "State (3,1): (-1, 0) Q=9.77\n",
            "State (3,1): (1, 0) Q=9.49\n",
            "State (3,1): (0, -1) Q=8.77\n",
            "State (3,1): (0, 1) Q=8.62\n",
            "-----------------\n",
            "State (3,2): (-1, 0) Q=9.51\n",
            "State (3,2): (1, 0) Q=8.80\n",
            "State (3,2): (0, -1) Q=9.50\n",
            "State (3,2): (0, 1) Q=9.05\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid size\n",
        "n, m = 4, 3\n",
        "\n",
        "# Rewards grid setup\n",
        "rewards = np.array([\n",
        "    [0, 10, -10],\n",
        "    [0, 0, 0],\n",
        "    [0, 0, 0],\n",
        "    [0, 0, 0]\n",
        "])\n",
        "\n",
        "# Initialize Q-values to zeros (Q-table)\n",
        "q_values = np.zeros((n, m, 4))\n",
        "\n",
        "# Possible actions that the agent can take: move up, down, left, or right.\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "# Discount factor gamma determines the agent's consideration for future rewards.\n",
        "gamma = 1.0\n",
        "\n",
        "# Learning rate alpha for Q-learning\n",
        "alpha = 0.5\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 10\n",
        "\n",
        "# Exploration rate (epsilon) for epsilon-greedy strategy\n",
        "epsilon = 0.1\n",
        "\n",
        "# Perform Q-learning with epsilon-greedy exploration\n",
        "for it in range(iterations):\n",
        "    #print(f\"Iteration {it + 1}\")\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            for action_index, action in enumerate(actions):\n",
        "                ni, nj = i + action[0], j + action[1]\n",
        "\n",
        "                # Ensure the agent stays within the grid\n",
        "                ni = max(0, min(n - 1, ni))\n",
        "                nj = max(0, min(m - 1, nj))\n",
        "\n",
        "                # If current state is (0,1) or (0,2), update the Q-values based on their respective rewards\n",
        "                if (i, j) == (0, 1):\n",
        "                    q_values[i, j, action_index] = 10\n",
        "                    continue\n",
        "                elif (i, j) == (0, 2):\n",
        "                    q_values[i, j, action_index] = -10\n",
        "                    continue\n",
        "\n",
        "                # Explore with probability epsilon or exploit with probability 1 - epsilon\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action_index = np.random.randint(0, len(actions))\n",
        "                    action = actions[action_index]\n",
        "\n",
        "                # Q-learning update rule:\n",
        "                # 1. Calculate the sample using the reward and the maximum Q-value of the next state\n",
        "                sample = rewards[i, j] + gamma * np.max(q_values[ni, nj])\n",
        "\n",
        "                # 2. Update Q-value using the Q-learning update rule (weighted average)\n",
        "                q_values[i, j, action_index] = (1 - alpha) * q_values[i, j, action_index] + alpha * sample\n",
        "\n",
        "                # Clip the Q-value to a maximum of 10 for (0,1) and a minimum of -10 for (0,2)\n",
        "                if i == 0 and j == 1:\n",
        "                    q_values[i, j, action_index] = min(q_values[i, j, action_index], 10.0)\n",
        "                elif i == 0 and j == 2:\n",
        "                    q_values[i, j, action_index] = max(q_values[i, j, action_index], -10.0)\n",
        "\n",
        "# Display final Q-values and corresponding actions for all states and paths\n",
        "print(f\"Final Q-values after {iterations} iteration(s)\")\n",
        "for i in range(n):\n",
        "    for j in range(m):\n",
        "        for action_index, action in enumerate(actions):\n",
        "            print(f\"State ({i},{j}): {action} Q={q_values[i, j, action_index]:.2f}\")\n",
        "        print(\"-----------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHf_Ay9pKw1a"
      }
    }
  ]
}